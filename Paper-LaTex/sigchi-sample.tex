\documentclass{sigchi}

% Use this command to override the default ACM copyright statement (e.g. for preprints). 
% Consult the conference website for the camera-ready copyright statement.


%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)
% \toappear{Permission to make digital or hard copies of all or part of this work for personal or classroom use is 	granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)


% Arabic page numbers for submission. 
% Remove this line to eliminate page numbers for the camera ready copy
% \pagenumbering{arabic}


% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage{url}      % llt: nicely formatted URLs
\usepackage{makecell}
\usepackage{changepage}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}


% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, 
% to give it a fighting chance of not being over-written, 
% since its job is to redefine many LaTeX commands.
\usepackage[pdftex]{hyperref}
\hypersetup{
pdftitle={SIGCHI Conference Proceedings Format},
pdfauthor={LaTeX},
pdfkeywords={SIGCHI, proceedings, archival format},
bookmarksnumbered,
pdfstartview={FitH},
colorlinks,
citecolor=black,
filecolor=black,
linkcolor=black,
urlcolor=black,
breaklinks=true,
}

% create a shortcut to typeset table headings
\newcommand\tabhead[1]{\small\textbf{#1}}


% End of preamble. Here it comes the document.
\begin{document}

\title{User-Defined Game Control with \\Smart Glasses in Public Space}

\numberofauthors{6}
\author{
  \alignauthor 1st Author Name\\
    \affaddr{Affiliation}\\
    \affaddr{Address}\\
    \email{e-mail address}\\
    \affaddr{Optional phone number}
  \alignauthor 2nd Author Name\\
    \affaddr{Affiliation}\\
    \affaddr{Address}\\
    \email{e-mail address}\\
    \affaddr{Optional phone number}    
  \alignauthor 3rd Author Name\\
    \affaddr{Affiliation}\\
    \affaddr{Address}\\
    \email{e-mail address}\\
    \affaddr{Optional phone number}
  \alignauthor 4rd Author Name\\
    \affaddr{Affiliation}\\
    \affaddr{Address}\\
    \email{e-mail address}\\
    \affaddr{Optional phone number}
  \alignauthor 5rd Author Name\\
    \affaddr{Affiliation}\\
    \affaddr{Address}\\
    \email{e-mail address}\\
    \affaddr{Optional phone number}
  \alignauthor 6rd Author Name\\
    \affaddr{Affiliation}\\
    \affaddr{Address}\\
    \email{e-mail address}\\
    \affaddr{Optional phone number}
}

\maketitle

\begin{abstract}
Without specific game controller and direct-touch, game control on Smart Glasses differs with existing console and mobile games. Although current game control set on Smart Glasses is explored by developers based on system limitation, the set is not reflective of user behavior.
To create better game control, we presented an user-defined game control study in public space to collect user behavior. In all, 2448 game control from 24 participants were logged, analyzed, and paired with think-aloud data for 17 commands performed with 3 interaction methods (On-Body, In-Air and Phone) and 2 glasses forms (Google Glass and Epson BT-100). 
Our findings indicate that users choose area relatively unobtrusive to perform the game control, and glasses form does influence how users creates game control. We also present a complete user-defined game control set with agreement scores and taxonomy. 
Our results will help designers create better game control sets informed by user behavior.
\end{abstract}

\keywords{
	Guides; instructions; author's kit; conference publications;
	keywords should be separated by a semi-colon. \newline
	\textcolor{red}{Optional section to be included in your final version, 
  but strongly encouraged.}
}

\category{H.5.m.}{Information Interfaces and Presentation (e.g. HCI)}{Miscellaneous}

See: \url{http://www.acm.org/about/class/1998/}
for more information and the full list of ACM classifiers
and descriptors. \newline
\textcolor{red}{Optional section to be included in your final version, 
but strongly encouraged. On the submission page only the classifiers’ 
letter-number combination will need to be entered.}

\section{Introduction}

%A. Review現有遊戲平台及操作方式。
%B. Smart Glass帶來新的可能性，跟傳統遊戲的區別。 
%C. 現有的Smart Glass Gaming有哪些，而且目前體驗很差。
%D. 我們認為造成體驗差的原因，跟我們的解法。


\section{Related Work}

    \subsection{Game Control}

    \subsection{Input Technology}
    Some working mobile systems have defined designer-made controll methods. These systems can be divided into two main categories, in-air gestures and on-body inputs. Kim et al. \cite{Kim:2012:DFI:2380116.2380139} developed a wrist-worn architecture, which supports discrete gesture recognition with reconstructing a 3D hand modal in the air. Similarly, Jing et al. \cite{Jing:2013:MRS:2541831.2541875} implemented \textsl{Magic Ring}, a finger ring shape input device using inertial sensor to detect subtle finger gestures. Cola\c{c}o et al. \cite{Colaco:2013:MCL:2501988.2502042} built a head-mounted display, \textsl{Mime}, sensing 3D gestures in front of users' eyes. 

    Harrison et al. \cite{Harrison:2011:OWM:2047196.2047255} created \textsl{OmniTouch}, a wearable depth-sensing and projected system that enables interactive multitouch applications on any surface of users' body. Moreover, Harrison et al. \cite{Harrison:2010:SAB:1753326.1753394} also presented \textsl{Skinput}, a technology that appropriates the human body for acoustic transmission, allowing the skin to be used as an input surface. Baudisch al. \cite{Gustafson:2011:IPL:2047196.2047233} illustrated a concept of imaginary interface with sensing several gestures on users' palms. Recently, Serrano et al. \cite{Serrano:2014:EUH:2611247.2556984} explored the use of \textsl{Hand-to-Face} input to interact with head-worn displays(HWD) and provided a set of guidlines for developing effective Hand-to-Face interactions based on two main facotrs they found, social acceptability and cultural effect.

    According to prior works mentioned above, there are many kinds of input methods and technologies to explore the best input for mobile devices, including Smart Glasses. However, these studies can not be compared with each others to show what the most suitble for users it is.  
    \subsection{Gaming in Public Space}

    \subsection{User-Defined Gesture}

\section{Developing a User-Defined Game Control Set}
%需要描述場域嘛？


    \subsection {Overview and Rationale}
    Playing game is a \textsl{user-computer dialogue}\cite{userComputer}, a conversation mediated by language of inputs and outputs. As in any dialogue, feedback is essential to conducting this conversation. When something is misunderstood between humans, it may be rephrased. The same is true for user-computer dialogues. Feedback, or lack thereof, either endorses or deters a player's action, causing the player to revise his or her mental model and possibly take a new action.

    In Developing a user-defined game control set, we did not want the limitation of input technology to influence users' behavior. Hence, we sought to remove the \textsl{gulf of execution}\cite{gulf} from the dialogue, creating, in seence, a monologue in which the player's behavior is always acceptable. This enables us to observe users' unrevised behavior, and drive system design to accommodate it.

    In view of this, we developed a user-defined game control set by having 24  participants perform game control with 2 Smart Glasses (Google Glass and Epson BT-100) in a public cafe. To avoid bias from visual hint\cite{Epps:2006:SHS:1125451.1125601}, no elements specific to Mobile, Console, PC games were shown. Similarly, no specific game title was assumed. Instead, participants acted in a simple blocks world of geometry shapes or basic human avatar. Each participant saw the effect of a game control (e.g., an object moving left and right) and was asked to perform the game control he or she though would cause that effect(e.g. moving the finger tip left and right in front of their chest). In another word, the effect of a game control is the \textsl{game task} to which the game control try to complete.

    Seven-teen game tasks were presented, and game controls were elicited for three different interaction methods (in-air gesture, on-body input, mobile phone interaction). The system did not attempt to sense the users' control input, but we use camera to record the whole control process. Participants used the think-aloud protocol and been interviewed about the control detail. They also supplied subjective preference ratings.

    The final user-defined game control set was developed in light of the \textsl{agreement} participants exhibited in choosing game control for each command\cite{Wobbrock:2005:MGS:1056808.1057043}. The more participants that used the same gesture for a given command, the more likely that gesture would be assigned to that command. In the end, our user-defined game control set emerged as a consistent collection founded on actual user behavior.
    %這邊要研究一下......why "surprisingly consistent"


    \subsection {Game Tasks}

    Casual game is one of the game categories with most players\cite{esa_ef_2014}, and it is shown high potential in public gaming\cite{Jurgelionis:2011:PET:2027456.2027462,Reis:2012:EMC:2405577.2405651,Biskupski:2014:DEB:2559206.2580097}. We choosed top 90 casual games\cite{TopGames} from existing platforms, including PCs, consoles and mobile games (30 games for each) by crawling and analyzing the sale and download count data from famous gaming websites\cite{appannie,VGChartz,Steam,GameStop}. We invited 3 experienced game developers to review these top 90 casual games. They found out 26 game tasks in total, and removed 9 tasks which were only used once in specific games. At last, we got a set of general casual game task(shown in Table \ref{tab:table1}) with 17 tasks, which can completely support 90\% of our top casual games. 

  \begin{table}
    \centering
    \begin{tabular}{|c|l|l|}
      \hline
      \tabhead{\#} &
      \multicolumn{1}{|p{0.4\columnwidth}|}{\centering\tabhead{Task}} &
      \multicolumn{1}{|p{0.4\columnwidth}|}{\centering\tabhead{Used in Famous Game}} \\
      \hline
      1 & Single select & Clash of Clans, Plague Inc.\\
      \hline
      2 & Vertical menu & Puzzle\&Dragon, PeggleHD \\
      \hline
      3 & Horizontal menu & Clash of Clans, PeggleHD\\
      \hline
      4 & Move left and right & Temple Run, Super Mario\\
      \hline
      5 & Move in 4 directions & 1943, RaidenX\\
      \hline
      6 & Switch 2 objects & Candy Crush, Bejeweled\\
      \hline
      7 & Move object to position & World of Goo, The Sim\\
      \hline
      8 & Draw a path & Draw Something, P\&D\\
      \hline
      9 & Throw an object (in-2D) & Angry Birds, PeggleHD\\
      \hline
      10 & Note highway & RockSmith, Deemo\\
      \hline
      11 & Rotate an object (Z-axis) & Zuma, PeggleHD \\
      \hline
      12 & Rotate an object (Y-axis) & Spore, The Sim\\
      \hline
      13 & Avatar jump & Temple Run, Super Mario\\
      \hline
      14 & Avatar 3D move & Spore, Tintin\\
      \hline
      15 & Avatar attack & Minecraft, Terraria\\
      \hline
      16 & Avatar squat & Temple Run, Minecraft\\
      \hline
      17 & 3D Viewport control & The Sim, Spore\\
      \hline

    \end{tabular}
    \caption{Summary of our general casual game task set. We named several famous games which uses these tasks.}
    \label{tab:table1}
  \end{table}


  \subsection {Participants}
  We recruited twenty-four participants from the mass population with equal sex ratio for our study. Their average age are 23.2 (\textsl{sd} = 2.72). All participants are right-handed and none of them had past experience with Smart Glasses usage. About their gaming experience, according to our investigation, 14 users were daily game players, 9 were weekly and 1 was monthly. Participants spent 1.36 hours (\textsl{sd} = 0.89) in average to play games one time. Moreover, 58\% of them indicated that their main gaming platforms were on mobile phones, 38\% were on PCs, and only 4\% were on consoles. Another important factor of gaming experience is the familiarity of game controllers. The result showed that, compared with joysticks, most of them were more familiar with keyboards, mouses and touch screens (see Figure~\ref{fig:figureFamiliarity}).
  %\begin{figure}[!h]
  %\centering
  %\includegraphics[width=0.9\columnwidth]{Platform}
  %\caption{With Caption Below, be sure to have a good resolution image
  %  (see item D within the preparation instructions).}
  %\label{fig:figurePlatform}
  %\end{figure} 
  %\begin{figure}[!h]
  %\centering
  %\includegraphics[width=0.9\columnwidth]{Frequency}
  %\caption{With Caption Below, be sure to have a good resolution image
  %  (see item D within the preparation instructions).}
  %\label{fig:figureFrequency}
  %\end{figure}
  \begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\columnwidth]{Familiarity}
  \caption{Users' game control familiarity.}
  \label{fig:figureFamiliarity}
  \end{figure}   

  \subsection {Glass Forms}
  There are many Smart Glasses with different screen sizes and screen placement on the current market. To observe the effect of distinct display designs upon the study result, our study conducted on two famous Smart Glasses, Epson BT-100 and Google Glass. The display of Epson BT-100 is located in front of the user's eyes with $960 \times 540$ resolution (equivalent of a 320" screen from 20 m away)\cite{BT100}. And Google Glass locates its display above the user's right eye with $640 \times 360$ resolution (equivalent of a 25" screen from 2.4 m away)\cite{GoogleGlass}.       

    \subsection {Interaction Methods}
    In our study, we asked users to define three control manners to satisfy three interaction requirements individually in each task. These three interaciton types, classified according to familiar interactions explored by previous works, were \textsl{In-Air}, \textsl{On-Body}, and \textsl{Phone}. \textsl{In-Air}, one of these types, was that users were asked to define a input method without touching any tangible surface. Another method was \textsl{On-Body}, which asked users to design a control action with touching any skin or accessories on their own bodies. The last method, \textsl{Phone}, was required users to create a game control by interacting with common always-available devices, mobile phones.    


    \subsection {Procedure}
    Users wore two different glasses (Epson BT-100 and Google Glass) and our software randomly presented 17 game tasks (Table \ref{tab:table1}) to participants. For each game task, participants performed a game control in 3 different interaction methods(in-air gesture, on-body input and phone interaction). And study conducted with a counterbalanced measures design with Glasses form and interaction method. After each game control, participants were shown a 5-point Likert scales concering subjective preference. With 24 participants, 17 game tasks, 2 glasses forms and 3 interaction methods, a total of $24 \times 17 \times 2 \times 3$ = 2448 game control were made. Of these, 11 were discarded due to participant confusion. 

\section{Results}

Our results include game control taxonomies, user-defined gesture sets, user rating, subjective responses, and qualitative observations for each interaction methods.

  \subsection{Preference Between Interaction Methods}
  Table \ref{tab:tablePreferenceInteractionMethod} shows the average rating of 3 interaction methods. Between three interaction types had a significant difference($F_{0.05}$(2, 2445)=4.61, P = .01). We found the user rating preference for in-air gesture was siginificant higher than phone interaction (P = .009). And we didn't find significant difference between in-air and on-body (P = .688).
  % , or on-body and phone interaction (P = .086). 
  According to our interview, general reasons about why did users give phone a lower score had been found. Reason were that users had to take out their phone from packet. Users thought it was not always available and was not hands-free compared to the other interaction methods in this study. Considering the article length of this paper and user preference mentioned above, our report will focus on the finding in-air gesture and on-body input.

  \begin{table}
    \centering
    \begin{tabular}{|l|l|l|l|l|}
      \hline
      \tabhead{Mothod} &
      \multicolumn{1}{|p{0.13\columnwidth}|}{\centering\tabhead{Mean}} &
      \multicolumn{1}{|p{0.13\columnwidth}|}{\centering\tabhead{Std.}} &
      \multicolumn{1}{|p{0.13\columnwidth}|}{\centering\tabhead{L.Bound}} &
      \multicolumn{1}{|p{0.13\columnwidth}|}{\centering\tabhead{U.Bound}} \\
      \hline
      In-Air & 3.81 & 0.90 & 3.75 & 3.87\\
      \hline
      On-Body & 3.77 & 0.81 & 3.72 & 3.83\\
      \hline
      Phone & 3.68 & 0.79 & 3.63 & 3.74\\
      \hline

    \end{tabular}
    \caption{Summary of user preference between 3 different interaction methods, it provides mean value, standard deviation, 95\% confidence interval for mean(Lower Bound and Upper Bound).}
    \label{tab:tablePreferenceInteractionMethod}
  \end{table}

  \subsection{Behavior with Different Glasses Forms}
  In our study, There are 1224 game control pairs with identical user, task and interaction method. We found 119 pairs of game control (9.72\% of all) were designed differently with distinct smart glasses forms. The influence of game control in each interation methods were 20.59\% for in-air gesture controls, 7.35\% for on-body input and 1.22\% for phone interaction. 

  While using in-air gesture as interaction method, users who designed distictive game control mentioned that they were eager to use direct-touch in front of their face with Epson BT-100. However, it is difficult to perform same control with Google Glass because of the small screen size (an in-air fat finger problem). On the other hand, reasons to use different controls with on-body and phone interaction methods are really random and users couldn't explain by their own.

  Although the form factor of smart glasses influenced the design of game control, the user preference ratings for user-defined game controls between 2 different glass forms were almost no difference($F_{0.05}$(1, 2446)=.36, P=.549).


  \subsection{Classification of Game Controls}
  As noted in related work, ???????? . However, no work has established a taxonomy of game control based on user behavior in public space to capture and describe the game design space.

   \begin{table}
    \centering
    \begin{tabular}{|c|l|c|}
      \hline
      \tabhead{\#} &
      \multicolumn{1}{|p{0.2\columnwidth}|}{\centering\tabhead{Task}} &
      \multicolumn{1}{|p{0.2\columnwidth}|}{\centering\tabhead{Kappa Value}} \\
      \hline
      1 & Single select & 0.863\\
      \hline
      2 & Vertical menu & 1.000\\
      \hline
      3 & Horizontal menu & 0.688\\
      \hline
      4 & Move left and right & 0.825\\
      \hline
      5 & Move in 4 directions & 1.000\\
      \hline
      6 & Switch 2 objects & 0.804\\
      \hline
      7 & Move object to position & 1.000\\
      \hline
      8 & Draw a path & 1.000\\
      \hline
      9 & Throw an object (in-2D) & 1.000\\
      \hline
      10 & Note highway & 0.697\\
      \hline
      11 & Rotate an object (Z-axis) & 0.867 \\
      \hline
      12 & Rotate an object (Y-axis) & 1.000\\
      \hline
      13 & Avatar jump & 0.867\\
      \hline
      14 & Avatar 3D move & 0.880\\
      \hline
      15 & Avatar attack & 1.000\\
      \hline
      16 & Avatar squat & 0.878\\
      \hline
      17 & 3D Viewport control & 0.878\\
      \hline
      & \bf{Average} & \bf{0.897}\\
      \hline

    \end{tabular}
    \caption{Interrater reliability for each task.}
    \label{tab:kappa}
  \end{table}


  \subsubsection{Taxonomy of Game Control}

  The authors manually classified each gesture along four dimensions: \emph{form}, \emph{nature}, \emph{binding}, and \emph{flow}. Within each dimension are multiple categories, shown in Table \ref{tab:taxonomy}.To control for interrater effects, an independent rater performed the same categorization using 170 trials data (random selected 10 trials for each tasks), Interrater reliability shown in table \ref{tab:kappa}. The lowest Kappa value, .688, is greater than .6 and is sufficient to establish validity. In addition, the average Kappa is .897. A Kappa value of .8 and higher is considered \textsl{almost perfect}.



    \begin{table}
    \centering
    \begin{adjustwidth}{-0.4cm}{}
    \begin{tabular}{|l|l|l|}
      \hline
      \multicolumn{3}{|p{\columnwidth}|}{\centering\tabhead{\textbf{Taxonomy of Game Controls}}}\\
      \Xhline{4\arrayrulewidth}
        \textbf{\em{Form}} & \em{finger} & Using finger to perform control.\\ \cline{2-3} 
        \textbf{\em{{\fontsize{0.3cm}{1em}\selectfont (In-Air)}}}  & \em{hand} & Using hand to perform control.\\ \cline{2-3} 
             & \em{head} & Using head to perform control.\\ \cline{2-3} 
             & \em{voice} & Using voice control.\\ 
      \Xhline{4\arrayrulewidth}
        \textbf{\em{Form}} & \em{palm} & Interact b.t. finger and palm. \\ \cline{2-3} 
        \textbf{\em{{\fontsize{0.3cm}{1em}\selectfont (On-Body)}}} & \em{fingers} & Interact b.t. fingers.\\ \cline{2-3} 
             & \em{leg} & Interact b.t. hand and leg.\\ \cline{2-3} 
             & \em{handback} & Interact b.t. finger and handback.\\ \cline{2-3} 
             & \em{forearm} & Interact b.t. finger and forearm.\\ \cline{2-3} 
             & \em{face} & Interact b.t. finger and face.\\ \cline{2-3} 
             & \em{wrist} & Interact b.t. finger and wrist.\\ \cline{2-3} 
             & \em{ring} & Interact with ring. \\ \cline{2-3} 
             & \em{watch} & Interact with watch.\\ \cline{2-3} 
             & \em{glasses} & Interact with glasses.\\ \cline{2-3} 
             & \em{necklace} & Interact with necklace.\\ 
      \Xhline{4\arrayrulewidth}
        \textbf{\em{Binding}} & \em{direct} & Directly control with screen. \\ \cline{2-3} 
             & \em{surface} & Absolute mapping screen to surface.\\ \cline{2-3} 
             & \em{independent} & No binding b.t. screen and control.\\
      \Xhline{4\arrayrulewidth}
        \textbf{\em{Nature}} & \em{symbolic} & Control visually depicts a symbol.\\ \cline{2-3} 
             & \em{physical} & Control acts physically on objects.\\ \cline{2-3} 
             & \em{metaphorical} & Control indicates a metaphor.\\ \cline{2-3} 
             & \em{abstract} & Control mapping is arbitrary.\\
      \Xhline{4\arrayrulewidth}
        \textbf{\em{Flow}} & \em{discrete} & Response occurs \em{after} the user acts.\\ \cline{2-3} 
             & \em{continuous} & Response occurs \em{before} the user acts.\\
      \hline
    \end{tabular}
    \caption{Taxonomy of game controls based on 2448 gestures. The abbreviation ``b.t.'' means ``between''.}
    \label{tab:taxonomy}
    \end{adjustwidth}
  \end{table}

  The scope of the \emph{form} dimension is applied separate to different interaction methods. There are 4 form categories with in-air gesture. \emph{Finger} is a special case of \emph{hand}, but it is worth distinguishing because of their similarity to mouse actions and direct-touch. There are 11 \emph{form} categories with on-body input. 4 of them (\emph{palm}, \emph{handback}, \emph{forearm}, \emph{wrist}) are operating with both hands. 2 of them (\emph{leg}, \emph{face}) are using single hand to interact with different body parts. \emph{fingers} is specific with single hand control with just interaction between fingers. rest of them (\emph{ring}, \emph{watch}, \emph{glasses}, \emph{necklace}) are interacting with gadgets.


  In the \emph{nature} dimension, \emph{symbolic} controls are visual depictions. For example, forming the victory pose in the air for selecting menu option 2, or forming a gun pose to throw an object. \emph{Physical} controls should ostensibly have the same effect with real world physical objects. \emph{Metaphorical} controls occur when a control acts on, with, or like something else. Examples are tracing a finger in circle to simulate a ``object ratating'', using two fingers to ``walk'' across the palm, pretending the palm as a trackpad to perform gestures. Of course, the control itself usually not enough to reveal its metaphorical nature; the answer lies in user's mental model which can be understood by our interview. Finally, \emph{abstract} gestures have no \emph{symbolic}, \emph{physical}, or \emph{metaphorical} connection to their referents. The mapping is arbitrary, which does not necessarily mean it is poor. Touch between thumb and index finger to perform ``avatar jump'', for example, would be an abstract control.

  \emph{Binding} dimension is categorized by the relationship between control area and smart glasses screen. \emph{Direct} means control with the screen region directly, like using in-air gesture in front of the face to draging virtual objects. A game control binding is \emph{Surface Mapping} if user is absolute mapping the screen into other surface and perform game control with it. Draging object on the palm, which is mapping to screen, to move object, for example, is a \emph{Surface mapping binding} control. 

  A game control's \emph{flow} is discrete if the control is performed delimited, recognized, and responded to as an event. An example is tapping a imaginary button in the air to perform ``avatar attack''. \emph{Flow} is continuous if ongoing recognition is required, such as during most of our participants' ``3D Camera Control'' with rotating imaginary camera by hands. 
 


 \subsubsection{Taxonometric Breakdown of Gestures in our Data}
We found that our taxonomy adequately describes even widely differing gestures made by our users. Figure \ref{fig:InAirTaxonomy} and \ref{fig:OnbodyTaxonomy} show for each dimension the percentage of gestures made within each category for all gestures in our study.

\emph{Form} of in-air gesture and on-body input are both dominated with hand related input, and we found that the form of on-body input are more complicate than in-air gesture. But the binding of on-body input is more consistent, almost 75\% controls are independent with screen. And we are surprisingly to see that no users design a \emph{direct-binding} or \emph{physical} control with on-body input. The portion of control's flow are similar in this two interaction methods.

 \begin{figure}[!h]
  \centering
  \includegraphics[width=1\columnwidth]{InAirTaxonomy}
  \caption{Percentage of game controls in each taxonomy category with in-air gesture.}
  \label{fig:InAirTaxonomy}
  \end{figure} 

 \begin{figure}[!h]
  \centering
  \includegraphics[width=1.02\columnwidth]{OnbodyTaxonomy}
  \caption{Percentage of game controls in each taxonomy category with on-body input.}
  \label{fig:OnbodyTaxonomy}
  \end{figure} 

  \subsection{User-Defined Game Control Set}
  At the heart of this work is the creation of a user-defined game control set with smart glasses in public space. This section gives the process by which the set was created and properties of the set. Unlike prior game control sets in traditional game platforms, this set is based on observed user behavior and joins user action to game tasks.


   \subsubsection{Agreement}
   After all 24 participants had provided game control for each game tasks with each glasses form and interaction methods. we grouped the game control within each task such that each group held identical controls. Group size was then used to compute an \emph{Agreement Scrore} that reflects, in a single number, the degree of consensus among participants. A task with .31 agreement score means that, random two users will has 31\% chance to perform the identical control action for this task. (The definition and formula of agreement score was refer to previous work \cite{Wobbrock:2005:MGS:1056808.1057043}.)

   Agreement for our study is graphed in Figure \ref{fig:Agreement}. The overall agreement for in-air and on-body inputs was $A_{Air}$=0.27 and $A_{Body}$=0.25, respectively. Comparing the agreement of in-air and on-body input, we found their pattern is really similar. The average difference of agreement between these two interaction methods is .056. It implys that the agreement score is influenced more by the game tasks than the interaction methods.


 \begin{figure}[!h]
  \centering
  \includegraphics[width=1\columnwidth]{Agreement}

  \caption{Agreement for each game tasks. The tasks are listed in the same order as they appear in Table \ref{tab:table1}.}
  \label{fig:Agreement}
  \end{figure} 

   %\subsubsection{Conflict and Coverage}

   %Howerver, where the same control action was used to perform different game tasks, a conflict occurred if the game consists the tasks at same time. In our case ,we found that ``Move in 4 Directions'',``Avatar 3D Move'' and ``Viewport Controls'' are assigned with same control action with on-body input(Fig?.?). According to the property of our top 90 casual games, the average task count for these game is 2.56 (std=1.07). In another word, there are about 1 to 4 game tasks in each game title. There is a low chance to perform these three game controls at same game and same time. To make our game control set reflects more consistent with user behavior, we decide to remain it conflict. 


   \subsubsection{Properties of the User-defined Gesture Sets}
   The user-defined game control set was developed by taking largest groups of identical control for each game task and assigning those groups' control to the task. 
   Our resulting user-defined game control set covers of 40.07\% and 41.32\% of game controls proposed with in-air gesture and on-body input respectively. Our user defined set is useful, therefore, not just for what it contains, but also for what it omits.

   With in-air gestures, Although we ask users to perform game control not limited with hand, the result shows that user still prefer using hand gestures over than voice controls, eye gesture and head tilting. And user would use direct-control if they have to do precise task, like selecting an object from many or moving an object to the specific position. Other tasks which are not so precise like select a single option from 4 or make a avatar jump, user will prefer using a in-direct control, like tapping 4 different area in-front of their chest or micro hand lift.

   With on-body inputs,



   \subsubsection{Taxonometric Breakdown of User-defined Game Controls}

  \subsection{Mental Model Observations}
    \subsubsection{Social Acceptance and Control Area}
    \subsubsection{Metaphor from Exisiting Game Control}

  \section{Discussion}
    \subsubsection{Users' and Designers' Gestures}
    \subsubsection{Implications for In-Air Gesture Technology}
    Before the study began, \cite{GoogleGlass, Colaco:2013:MCL:2501988.2502042} both supplied their own in-air gesture sets to increase the diversity of their input. However, the result showed that 63\% users do not prefer to perform in-air gestures in front of their faces in the public space because of social acceptance and physical tiring issues metioned by feedback. Therefore, if developers of head worn devices want to implement in-air gestures for input, they will need to have capablity of sensing gestures in a wide range of areas near users. Take CV-based sensing technologies as example, they can use wide-angle lens or fish-eye lens to implement a gesture set to cater to users' preference.   
  \begin{figure}[!h]
  \centering
  \includegraphics[width=1\columnwidth]{InAirControlArea.pdf}
  \caption{Form summary of in-air gesture. (A)Using finger to perform in-air gesture. (B)Using full hand to perform in-air gesture. (C)Using head-tilting or rotating to perform game control. (D)Perfrom game control with eyes-gesture. (E)The portion of in-air ``hand'' gesture control area. Half of in-air gestures (49\%) were performed in front of their chest, 14\% were in front of belly or below. Only 37\% gestures were performed in front of their face.}
  \label{fig:figureInAirPorpotion}
  \end{figure}
  \subsubsection{Implications for On-Body Input Technology}

  \subsubsection{Implications for Game Control Design}

 \begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\columnwidth]{OnBodyForms}
  \caption{The top 6 on-body input forms. (A)Interact between finger and palm. (B)Interact with ring. (C)Interact between fingers.(D)Interact between finger and leg. (E)Interact between finger and hand back. (F)Interact with watch.}
  \label{fig:figureOnBodyPorpotion}
  \end{figure}   

  \subsubsection{Implications for User Interfaces}
  \subsubsection{Limitation and Next Steps}
    %Glass Forms 只有兩種
    %Culture問題?
    %Interaction with 桌椅？


\section{Conclusion}


%It is important that you write for the SIGCHI audience.  Please read
%previous years' Proceedings to understand the writing style and
%conventions that successful authors have used.  It is particularly
%important that you state clearly what you have done, not merely what
%you plan to do, and explain how your work is different from previously
%published work, i.e., what is the unique contribution that your work
%makes to the field?  Please consider what the reader will learn from
%your submission, and how they will find your work useful.  If you
%write with these questions in mind, your work is more likely to be
%successful, both in being accepted into the Conference, and in
%influencing the work of our field.

\section{Acknowledgments}

%We thank CHI, PDC and CSCW volunteers, and all publications support
%and staff, who wrote and provided helpful comments on previous
%versions of this document.  Some of the references cited in this paper
%are included for illustrative purposes only.  \textbf{Don't forget
%to acknowledge funding sources as well}, so you don't wind up
%having to correct it later.

% Balancing columns in a ref list is a bit of a pain because you
% either use a hack like flushend or balance, or manually insert
% a column break.  http://www.tex.ac.uk/cgi-bin/texfaq2html?label=balance
% multicols doesn't work because we're already in two-column mode,
% and flushend isn't awesome, so I choose balance.  See this
% for more info: http://cs.brown.edu/system/software/latex/doc/balance.pdf
%
% Note that in a perfect world balance wants to be in the first
% column of the last page.
%
% If balance doesn't work for you, you can remove that and
% hard-code a column break into the bbl file right before you
% submit:
%
% http://stackoverflow.com/questions/2149854/how-to-manually-equalize-columns-
% in-an-ieee-paper-if-using-bibtex
%
% Or, just remove \balance and give up on balancing the last page.
%
\balance

%\section{References format}
%References must be the same font size as other body text.
% REFERENCES FORMAT
% References must be the same font size as other body text.

\bibliographystyle{acm-sigchi}
\bibliography{sample}
\end{document}
