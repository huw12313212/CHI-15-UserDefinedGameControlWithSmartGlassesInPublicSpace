\documentclass{sigchi}

% Use this command to override the default ACM copyright statement (e.g. for preprints). 
% Consult the conference website for the camera-ready copyright statement.


%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)
% \toappear{Permission to make digital or hard copies of all or part of this work for personal or classroom use is 	granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)


% Arabic page numbers for submission. 
% Remove this line to eliminate page numbers for the camera ready copy
% \pagenumbering{arabic}


% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage{url}      % llt: nicely formatted URLs
\usepackage{makecell}
\usepackage{changepage}
\usepackage{mathtools}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}


% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, 
% to give it a fighting chance of not being over-written, 
% since its job is to redefine many LaTeX commands.
\usepackage[pdftex]{hyperref}
\hypersetup{
pdftitle={SIGCHI Conference Proceedings Format},
pdfauthor={LaTeX},
pdfkeywords={SIGCHI, proceedings, archival format},
bookmarksnumbered,
pdfstartview={FitH},
colorlinks,
citecolor=black,
filecolor=black,
linkcolor=black,
urlcolor=black,
breaklinks=true,
}

% create a shortcut to typeset table headings
\newcommand\tabhead[1]{\small\textbf{#1}}


% End of preamble. Here it comes the document.
\begin{document}


\title{User-Defined Game Control \\for Smart Glasses in Public Space}


%\author{\alignauthor Chun-Yen Hsu, Ying-Chao Tung, Han-Yu Wang, \\Silvia Chyou, Jer-Wei Lin, Mike Y. Chen\\
%\affaddr{Mobile and HCI Research Lab, National Taiwan University} \\ 
%\email{\{hcythomas0125,tony61507,huw12313212,silvia.chyou,evin92\}@gmail.com,\\ mikechen@csie.ntu.edu.tw}
%}


\maketitle




\begin{abstract}


Without a controller or direct-touch, game control on smart glasses differs from existing console and mobile games. Although current game control on smart glasses is explored by developers based on system capability, the control set is not reflective of user behavior. We present a user-defined game control study in public space to collect user behavior without influence from system limitation. In all, 2448 game controls from 24 participants were logged, analyzed, and paired with think-aloud data for 17 commands performed with 3 interaction methods (\emph{In-Air}, \emph{On-Body} and \emph{Phone}) and 2 different smart glasses (\emph{Google Glass} and \emph{Epson BT-100}). Our findings indicate that the user preference with \emph{In-Air} interaction is significant higher than \emph{Phone} controller, that users prefer a relatively unobtrusive control area with concerning social acceptance, and that different form factor of smart glasses does influence how users create control actions. We also present a complete user-defined game control set with agreement scores and taxonomy. Our results will help designers create better game controls informed by user behavior.


% Game control on smart glasses differs from existing console and mobile games due to the lack of physical game controllers or direct-touch. Although current game control on smart glasses is explored by developers based on system capability, the set of controls is not reflective of user behavior. 
% To create better game control, we presented a user-defined game control study in public space to collect user behavior. In all, 2448 game controls from 24 participants were logged, analyzed, and paired with think-aloud data for 17 commands performed with 3 interaction methods (\emph{In-Air}, \emph{On-Body} and \emph{Phone}), and On-Body method is inclusive of wearble devices' control. The study used 2 different forms of smart glasses (Google Glass and Epson BT-100) and users don't need to consider the system limitation. We find that users prefer using \emph{In-Air} and \emph{On-Body} inputs to perform the game controls in a relatively unobtrusive area, and that different forms of smart glasses do influence how users create control actions. Users will choose less noticeable control methods because of social acceptance. We also present a complete user-defined game control set with agreement scores and taxonomy data. 
% Our results will help designers create better game control based on user behavior.

% Abstract 的 Take home note 不夠明顯
% Abstract 強調 System Limitation 不再考慮範圍內
% Abstract On Body要看出來 Wearble Diveces的操作
% 1.使用者在Public Space Gaming時 比較prefer In-Air 及 On-Body Interaction方式。
% 2.使用者會因為Social Acceptable的原因而選擇較不引人注目的操作方式。
% 3.眼鏡的Form Factor （螢幕大小&位置) 影響了使用者對In-Air Gesture的設計。

\end{abstract}


\keywords{
  Game, input, control, smart glasses, guessability, think-aloud, user-defined, public space, pervasive gaming, wearable.
	% Guides; instructions; author's kit; conference publications;
	% keywords should be separated by a semi-colon. \newline
	% \textcolor{red}{Optional section to be included in your final version, 
 %  but strongly encouraged.}
}

\category{H.5.2.}{Information Interfaces and Presentation}
: User Interfaces - \emph{Interaction styles, evaluation/methodology, user-centered design.}
% See: \url{http://www.acm.org/about/class/1998/}
% for more information and the full list of ACM classifiers
% and descriptors. \newline
% \textcolor{red}{Optional section to be included in your final version, 
% but strongly encouraged. On the submission page only the classifiers’ 
% letter-number combination will need to be entered.}



\section{Introduction}

 \begin{figure}[!h]
  \centering
  \includegraphics[width=1\columnwidth]{TopFigure2.pdf}
  \caption{In a Starbucks cafe, a user performing an in-air gesture to select a single object after being prompted by an animation demonstrating the selecting effect (a pop out animation).}
  \label{fig:TopFigure}
  \end{figure} 

%The invention of smart glasses offers more 
%Emerging wearable computing devices atrract extensive attention worldwide. 
Smart glasses offer the opportunity to achieve always available gaming and instant playing. We can play a game without any setup. Smart glasses, such as Google Glass and the Epson BT-100, are computerized eyeglasses and are usually equipped with various sensors, such as a camera, gyroscope, accelerometer and GPS, to enrich their capabilities. 
Without a physical game controller and a touch-screen, game control on smart glasses differs from existing consoles and mobile games. Currently no general game control set for smart glass based gaming exists yet. However, Some developers explored several game control sets based on system capability. Take mini games \cite{MiniGames} provided by Google for example, ``Balance'' uses the gyroscope to capture head gestures as game input. With the built-in microphone,``Clay Shooter'' utilizes the user's voice to trigger a gun shot, and ``Shape Splitter'' uses in-air gestures in front of the glass's camera.  
Nevertheless, what kinds of control actions do regular users make naturally? In the users' minds, what characteristics are important for such game controls? Will they prefer direct-touch-like control or just a physical controller? How consistent are game controls employed by different users for the same game task? 
Although designers may organized their game controls in a principled, logical fashion, user behavior is rarely so systematic. As McNeill \cite{HandAndMind} writes in his laborious study of human discursive gesture, ``Indeed, the important thing about gestures is that they are \emph{not} fixed. They are free and reveal the idiosyncratic imagery of thought''.


To investigate these idiosyncrasies, we employ a guessability study methodology \cite{Wobbrock:2005:MGS:1056808.1057043} that presents the \emph{effects} of game controls on participants and elicits the \emph{causes} meant to invoke them in a real-world environment. By interview and video analysis, we obtain rich qualitative data that illuminates users' mental models. By collecting preference score with different interaction methods and forms of glasses, 
we find that users have a significant higher preference to use \emph{In-Air} interactions than \emph{Mobile Phone}.
 And we find that the glass form does influence on how users create game controls. The final result is a detailed picture of user-defined game controls and the mental models that accompany them. Although some prior works explored smart glass input \cite{Colaco:2013:MCL:2501988.2502042,Serrano:2014:EUH:2611247.2556984}, our work is the first one to utilize users, rather than principles, in the development of a smart glass gaming control set. Moreover, we explicitly recruited regular people without prior experience using smart glasses expecting that they would behave with and reason about interactive smart glasses differently than designers and system builders.

This work contributes the following to glass gaming research: 
(1) a quantitative and qualitative characterization of user-defined game controls, including a taxonomy, 
(2) a user-defined game control set, 
(3) insight into users' mental models when making game controls in public space with understanding of implications for mobile-input technology. 
Our results will help designers create better game controls informed by user behavior.


% and were interviewed about the control detail
%To reflective of user behavior.
%Although current game control set on smart glasses is explored by capability 

%A. Review現有遊戲平台及操作方式。
%B. Smart Glass帶來新的可能性，跟傳統遊戲的區別。 
%C. 現有的Smart Glass Gaming有哪些，而且目前體驗很差。
%D. 我們認為造成體驗差的原因，跟我們的解法。


\section{Related Work}

    \subsection{Game Control}
    There are two main research fields related to game controls. One is comparing existing game controls with player experience. For examples, Cairns et al.\cite{Cairns:2014:ICI:2556288.2557345} looked at how the naturalness of the game controls influences the experience of immersion in mobile games; The study of Birk et al.\cite{Birk:2013:CYG:2470654.2470752} showed that there were a number of controller effects on player experience and in-game player personality; Lankes et al.\cite{Lankes:2012:CVC:2367616.2367629} investigated the relationship between the feeling of being in control in a game situation and the interaction complexity; Park et al.\cite{Park:2014:HFS:2556288.2557091} examined the characteristics of a speed-based exergame controller that bears on human factors; Cheema et al.\cite{Cheema:2011:WWT:2159365.2159407} explored aspects of player's experience in first person games that use 3D gestures for interaction.

    Another field is to explore new kinds of game control inputs. Previous works\cite{Ekman:2008:IEU:1358628.1358820,Vickers:2013:PLT:2531922.2514856,Sundstedt:2010:GGU:1837101.1837106,Smith:2006:UEM:1178823.1178847} shows the possibility to use eye gestures as game inputs; Christian et al.\cite{Christian:2014:VSI:2559206.2580103} and Yim et al.\cite{Yim:2008:EDD:1496984.1497033} provided novel techniques for users to interact with games by head-gesture; Harada et al.\cite{Harada:2011:VGI:2042053.2042059} and Sporka et al. \cite{Sporka:2006:NIS:1168987.1169023} both indicated that the voice input greatly expanded the scope of games that could be played hands-free and just counted on voice input; Baba et al.\cite{Baba:2007:VGU:1278280.1278285} presented a game prototype which treated skin contact as controller input; Nacke et al.\cite{Nacke:2011:BGD:1978942.1978958} even considered using biofeedback (including EMG, EDA, EKG, RESP, TEMP) as game input methods.


    \subsection{Mobile Input Technology}
    Some works related to mobile systems had defined designer-made control methods. These systems could be divided into two main categories, in-air gestures and on-body inputs. Kim et al.\cite{Kim:2012:DFI:2380116.2380139} developed a wrist-worn architecture, which supports discrete gesture recognition with reconstructing a 3D hand model in the air. Similarly, Jing et al.\cite{Jing:2013:MRS:2541831.2541875} implemented \textsl{Magic Ring}, a finger ring shaped input device using inertial sensors to detect subtle finger gestures; Cola\c{c}o et al.\cite{Colaco:2013:MCL:2501988.2502042} built a head-mounted display, \textsl{Mime}, sensing 3D gestures in front of the user's eyes. 

    Harrison et al.\cite{Harrison:2011:OWM:2047196.2047255} created \textsl{OmniTouch}, a wearable depth-sensing and projected system that enables interactive multitouch applications on any surface of the user's body. Moreover, \textsl{Skinput}\cite{Harrison:2010:SAB:1753326.1753394}, a technology that appropriates the human body for acoustic transmission, and allows the skin to be used as an input surface. Baudisch et al.\cite{Gustafson:2011:IPL:2047196.2047233} illustrated a concept of imaginary interface with sensing several gestures on the user's palms. Recently, Serrano et al.\cite{Serrano:2014:EUH:2611247.2556984} explored the use of \textsl{Hand-to-Face} input to interact with head-worn displays(HWD) and provided a set of guidelines for developing effective Hand-to-Face interactions based on two main factors they found, social acceptability and cultural effect.

    \subsection{User Elicitation Studies}
    User-elicitation studies are a specific type of participatory design methodology that involves end-users in the design of control-sets\cite{Good:1984:BUI:358274.358284,Morris:2012:WWI:2396636.2396651}. These studies had been used to design user interfaces of various types including multi-touch gestures on small and large surfaces\cite{Anthony:2012:IRC:2396636.2396671,Epps:2006:SHS:1125451.1125601,Wobbrock:2009:UGS:1518701.1518866,Findlater:2012:BQA:2207676.2208660} and multi-modal interactions \cite{Morris:2012:WWI:2396636.2396651,Valdes:2014:EDS:2611222.2557373}. There is also some evidence that user-defined control sets are more complete than those sets defined solely by experts\cite{Anthony:2012:IRC:2396636.2396671,Pyryeskin:2012:CEG:2396636.2396638,Wobbrock:2009:UGS:1518701.1518866}.

    In a user-elicitation study, users were shown referents (an action's effects) and were asked to demonstrate the interactions that resulted in a given referent\cite{Wobbrock:2009:UGS:1518701.1518866}. In this work, we draw upon the user-elicitation methodology to identify user expectations and suggestions for smart glass gaming.

\section{Developing a User-Defined Game Control Set}
%需要描述場域嘛？


    \subsection {Overview and Rationale}
    Playing a game is a \textsl{user-computer dialogue}\cite{userComputer}, a conversation mediated by a language of inputs and outputs. As in any dialogue, feedback is essential to conduct this conversation. When something is misunderstood between humans, it may be rephrased. The same is true for user-computer dialogues. Feedback, or lack thereof, either endorses or deters a player's action, causing the player to revise his or her mental model and possibly take a new action.

    In developing a user-defined game control set, we did not want the limitation of input technology to influence the user's behavior. Hence, we sought to remove the \textsl{gulf of execution}\cite{gulf} from the dialogue, creating, in essence, a monologue in which the player's behavior is always acceptable. This enables us to observe the user's unrevised behavior, and drive system design to accommodate it.

    In view of this, we developed a user-defined game control set by having 24 participants perform game tasks with 2 smart glasses (Google Glass and Epson BT-100) in a public cafe. To avoid bias from visual hints\cite{Epps:2006:SHS:1125451.1125601}, no elements specific to Mobile, Console or PC games were shown. Similarly, no specific game title was assumed. Instead, participants acted in a simple blocks world of geometry shapes or in the shape of a basic human avatar. Each participant saw the effect of a game control (e.g., an object moving left and right) and was asked to perform the game control he or she thought would use to cause that effect (e.g. moving their finger tip from left to right in front of their chest). 

    Seventeen game tasks were presented, and game controls were elicited for three different interaction methods (\emph{In-Air}, \emph{On-Body}, \emph{Phone}). The system did not attempt to sense the user's control input, but we used a camera to record the whole process. Participants used the think-aloud protocol and were interviewed about the control details. They also provided subjective preference ratings.

    The final user-defined game control set was developed in light of the \textsl{agreement} found in the participants' preferred control action for each game task\cite{Wobbrock:2005:MGS:1056808.1057043}. The more participants that used the same action for a given task, the more likely that control action would be assigned to the task. In the end, our user-defined game control set emerged as a surprisingly consistent collection founded on actual user behavior.
    %這邊要研究一下......why "surprisingly consistent"


    \subsection {Game Tasks}

    Casual game is one of the game categories with the most players\cite{esa_ef_2014}, and it is shown high potential in public gaming\cite{Jurgelionis:2011:PET:2027456.2027462,Reis:2012:EMC:2405577.2405651,Biskupski:2014:DEB:2559206.2580097}. We chose top 90 casual games\cite{TopGames} from existing platforms, including PCs, consoles and mobile games (30 games for each) by crawling and analyzing the sale and download count data from famous gaming websites\cite{appannie,VGChartz,Steam,GameStop}. We invited 3 experienced game developers to review these top 90 casual games. In these games they found 26 game tasks in total, and removed 9 tasks which were only used once in specific games. Finally, we got a set of general casual game task (shown in Table \ref{tab:table1}) with 17 tasks, which can completely support 90\% of our top casual games. 

  \begin{table}
    \centering
    \begin{tabular}{|c|l|l|}
      \hline
      \tabhead{\#} &
      \multicolumn{1}{|p{0.4\columnwidth}|}{\centering\tabhead{Task}} &
      \multicolumn{1}{|p{0.45\columnwidth}|}{\centering\tabhead{Used in Famous Game}} \\
      \hline
      1 & Select Single & Clash of Clans, Plague Inc.\\
      \hline
      2 & Vertical menu & Puzzle\&Dragon, PeggleHD \\
      \hline
      3 & Horizontal menu & Clash of Clans, PeggleHD\\
      \hline
      4 & Move left and right & Temple Run, Super Mario\\
      \hline
      5 & Move in 4 directions & 1943, RaidenX\\
      \hline
      6 & Switch 2 objects & Candy Crush, Bejeweled\\
      \hline
      7 & Move object to position & World of Goo, The Sim\\
      \hline
      8 & Draw a path & Draw Something, P\&D\\
      \hline
      9 & Throw an object (in-2D) & Angry Birds, PeggleHD\\
      \hline
      10 & Note highway & RockSmith, Deemo\\
      \hline
      11 & Rotate an object (Z-axis) & Zuma, PeggleHD \\
      \hline
      12 & Rotate an object (Y-axis) & Spore, The Sim\\
      \hline
      13 & Avatar jump & Temple Run, Super Mario\\
      \hline
      14 & Avatar 3D move & Spore, Tintin\\
      \hline
      15 & Avatar attack & Minecraft, Terraria\\
      \hline
      16 & Avatar squat & Temple Run, Minecraft\\
      \hline
      17 & Viewport control & The Sim, Spore\\
      \hline

    \end{tabular}
    \caption{Summary of our general casual game task set. We named several famous games which uses these tasks.}
    \label{tab:table1}
  \end{table}


  \subsection {Participants}
  We recruited twenty-four participants with an equal male-female ratio for our study. Their average age was 23.2 (\textsl{sd} = 2.72). All participants are right-handed and none of them had past experience with smart glasses usage. About their gaming experience, according to our investigation, 14 users were daily game players, 9 were weekly players and 1 was a monthly player. Participants spent 1.36 hours (\textsl{sd} = 0.89) on average to play games one time. Moreover, 58\% of them indicated that their main gaming platforms were mobile phones, 38\% were on PCs, and only 4\% were on consoles. Another important factor of gaming experience is the familiarity of game controllers. The results showed that, compared with joysticks, most of them were more familiar with keyboards, mouses and touch screens (see Figure~\ref{fig:figureFamiliarity}).
  %\begin{figure}[!h]
  %\centering
  %\includegraphics[width=0.9\columnwidth]{Platform}
  %\caption{With Caption Below, be sure to have a good resolution image
  %  (see item D within the preparation instructions).}
  %\label{fig:figurePlatform}
  %\end{figure} 
  %\begin{figure}[!h]
  %\centering
  %\includegraphics[width=0.9\columnwidth]{Frequency}
  %\caption{With Caption Below, be sure to have a good resolution image
  %  (see item D within the preparation instructions).}
  %\label{fig:figureFrequency}
  %\end{figure}
  \begin{figure}[!h]
  \centering
  \includegraphics[width=1\columnwidth]{Familiarity.pdf}
  \caption{Users' game control familiarity.}
  \label{fig:figureFamiliarity}
  \end{figure}   

  \subsection {Environment}
  According to the previous works\cite{Wiliamson:2011:MMI:2070481.2070551,Williamson:2013:MEM:2522848.2522874,Montero:2010:YUS:1851600.1851647,Rico:2010:UGM:1753326.1753458}, the social acceptability of mobile-input was influenced by whether participants believed a bystander could interpret the intention of the control action. Therefore, to provide a game control set suited for a real-world environment, we chose a Starbucks cafe near our college. The visitor flow of the cafe, on average, was 72.5 persons per hour. In our investigation, participants indicated that the cafe was comparatively a public space with average 4.17 points (\textsl{sd}=0.65) on a 5-point Likert Scale about degree of field publicity (1 means very private, 5 means very public).    

  \subsection {Form Factor of Glasses}
  There are many smart glasses with different screen sizes and screen placements on the current market. To observe the effect of distinct display designs upon the study result, our study was conducted on two well known smart glasses with different form factors, the Epson BT-100 and Google Glass (see Figure \ref{fig:Glasses}). The display of the Epson BT-100 is located in front of the user's eyes with $960 \times 540$ resolution (equivalent of a 320" screen from 20 m away)\cite{BT100}. And Google Glass locates its display above the user's right eye with $640 \times 360$ resolution (equivalent of a 25" screen from 2.4 m away)\cite{GoogleGlass}.  

  \begin{figure}[!h]
  \centering
  \includegraphics[width=1\columnwidth]{Glasses.pdf}
  \caption{(A) Google Glass, (B) Epson BT-100.}
  \label{fig:Glasses}
  \end{figure}  


    \subsection {Interaction Methods}
    In our study, we asked users to define three control manners to satisfy three interaction requirements individually in each task. These three interaction types, classified according to familiar interactions explored by previous works, were \textsl{In-Air}, \textsl{On-Body}, and \textsl{Phone}. \textsl{In-Air}, one of these types, was that users were asked to define an input method without touching any tangible object, such as, moving eyeballs, rotating their heads, voice control or in-air gestures. Another method was \textsl{On-Body}, which asked users to design a control action by touching any skin, clothes or accessories on their own bodies. The last method, \textsl{Phone}, required users to create a game control by interacting with common always-available devices, mobile phones.    


    \subsection {Procedure}
    Participants wore two different glasses (Epson BT-100 and Google Glass) and our software randomly presented 17 game tasks (Table \ref{tab:table1}) to participants. For each game task, participants performed a control action in 3 different interaction methods (\emph{In-Air}, \emph{On-Body} and \emph{Phone} interaction). The study was conducted using a counterbalanced measures design, alternating the glass's form and the interaction method. After each game control, participants were shown a 5-point Likert scale concerning subjective preference. With 24 participants, 17 game tasks, 2 glass forms and 3 interaction methods, a total of $24 \times 17 \times 2 \times 3$ = 2448 game controls were made. Of these, 11 were discarded due to participant confusion. 

\section{Results}

Our results include game control taxonomies, user-defined game control sets, user rating, subjective responses, and qualitative observations for each interaction method.

  \subsection{Preference Between Interaction Methods}
  Table \ref{tab:tablePreferenceInteractionMethod} shows the average rating of 3 interaction methods. Three interaction types had a significant rating difference ($F_{0.05}$(2, 2445)=4.61, P = .01). We found that the user rating preference for \emph{In-Air} was significantly higher than for \emph{Phone} (P = .009). And we didn't find a significant difference between \emph{In-Air} and \emph{On-Body} (P = .688).
  % , or on-body and phone interaction (P = .086). 
  In the interview conducted after the rating, we asked why users gave \emph{Phone} a lower preference score.
  The general reason was that users had to take out their phone from their pocket. Users thought it was not always available and was not hands-free compared to the other interaction methods in this study. Considering the user preference, our report will focus on the results of \emph{In-Air} and \emph{On-Body}.

  \begin{table}
    \centering
    \begin{tabular}{|l|l|l|l|l|}
      \hline
      \tabhead{Method} &
      \multicolumn{1}{|p{0.13\columnwidth}|}{\centering\tabhead{Mean}} &
      \multicolumn{1}{|p{0.13\columnwidth}|}{\centering\tabhead{Std.}} &
      \multicolumn{1}{|p{0.13\columnwidth}|}{\centering\tabhead{L.Bound}} &
      \multicolumn{1}{|p{0.13\columnwidth}|}{\centering\tabhead{U.Bound}} \\
      \hline
      In-Air & 3.81 & 0.90 & 3.75 & 3.87\\
      \hline
      On-Body & 3.77 & 0.81 & 3.72 & 3.83\\
      \hline
      Phone & 3.68 & 0.79 & 3.63 & 3.74\\
      \hline

    \end{tabular}
    \caption{Summary of user preference between 3 different interaction methods, it provides mean value, standard deviation, 95\% confidence interval for mean(Lower Bound and Upper Bound).}
    \label{tab:tablePreferenceInteractionMethod}
  \end{table}

  \subsection{Behavior with Different Form Factor of Glasses}
  In our study, for each of the two glasses, there are 1224 game control pairs with the user, task and interaction method. We found 119 pairs of game control (9.72\% of all) were designed differently with distinct smart glasses forms. The influence of game control in each interaction method was 20.59\% for \emph{In-Air}, 7.35\% for \emph{On-Body} and 1.22\% for \emph{Phone}. 

  While using \emph{In-Air} as interaction method, users who designed distinctive game control mentioned that they were eager to use a gesture best explained by the term \emph{``In-Air Direct-Touch''} in front of their face with the Epson BT-100. However, it is difficult to perform same control with Google Glass because of the small screen size (an in-air fat finger problem). On the other hand, users' reasons to define different control input with different glasses for \emph{On-Body} and \emph{Phone} interaction methods seem to be random as users were unable to motivate their choice to define a different control input.

  Although the form factor of smart glasses influenced the design of game controls, there was almost no difference in the user preference ratings for user-defined game controls between the 2 different glasses: ($F_{0.05}$(1, 2446)=.36, P=.549).


  \subsection{Classification of Game Controls}
  %As noted in related work, ???????? . Ho however, no work has established a taxonomy of game control based on user behavior in public space to capture and describe the game design space.

   \begin{table}
    \centering
    \begin{tabular}{|c|l|c|}
      \hline
      \tabhead{\#} &
      \multicolumn{1}{|p{0.4\columnwidth}|}{\centering\tabhead{Task}} &
      \multicolumn{1}{|p{0.2\columnwidth}|}{\centering\tabhead{Kappa Value}} \\
      \hline
      1 & Select Single & 0.863\\
      \hline
      2 & Vertical menu & 1.000\\
      \hline
      3 & Horizontal menu & 0.688\\
      \hline
      4 & Move left and right & 0.825\\
      \hline
      5 & Move in 4 directions & 1.000\\
      \hline
      6 & Switch 2 objects & 0.804\\
      \hline
      7 & Move object to position & 1.000\\
      \hline
      8 & Draw a path & 1.000\\
      \hline
      9 & Throw an object (in-2D) & 1.000\\
      \hline
      10 & Note highway & 0.697\\
      \hline
      11 & Rotate an object (Z-axis) & 0.867 \\
      \hline
      12 & Rotate an object (Y-axis) & 1.000\\
      \hline
      13 & Avatar jump & 0.867\\
      \hline
      14 & Avatar 3D move & 0.880\\
      \hline
      15 & Avatar attack & 1.000\\
      \hline
      16 & Avatar squat & 0.878\\
      \hline
      17 & Viewport control & 0.878\\
      \hline
      & \bf{Average} & \bf{0.897}\\
      \hline

    \end{tabular}
    \caption{Interrater reliability for each task.}
    \label{tab:kappa}
  \end{table}


  \subsubsection{Taxonomy of Game Control}
       
  As the authors we manually classified the control actions along four dimensions: \emph{form}, \emph{binding}, \emph{nature}, and \emph{flow}. Within each dimension there are multiple categories as shown in Table \ref{tab:taxonomy}. To verify the objectivity (or inter-rater reliability), we invited an independent rater who performed the same categorization using 170 trials (10 trials were randomly selected for each task). The inter-rater reliability is shown in Table \ref{tab:kappa}. The lowest Kappa value, .688, is greater than .6, which is rated as \textsl{substantial} and thus is sufficient to establish the validity of the categorization. In addition, the average Kappa value is .897. A Kappa value of .8 and higher is considered \textsl{almost perfect}\cite{kappavalue}.



    \begin{table}
    \centering
    \begin{adjustwidth}{-0.4cm}{}
    \begin{tabular}{|l|l|l|}
      \hline
      \multicolumn{3}{|p{1.06\columnwidth}|}{\centering\tabhead{\textbf{Taxonomy of Game Controls}}}\\
      \Xhline{4\arrayrulewidth}
        \textbf{\em{Form}} & \em{finger} & Using finger to perform control.\\ \cline{2-3} 
        \textbf{\em{{\fontsize{0.3cm}{1em}\selectfont (In-Air)}}}  & \em{hand} & Using hand to perform control.\\ \cline{2-3} 
             & \em{head} & Using head to perform control.\\ \cline{2-3} 
             & \em{voice} & Using voice control.\\ 
      \Xhline{4\arrayrulewidth}
        \textbf{\em{Form}} & \em{palm} & I.b. finger and palm. \\ \cline{2-3} 
        \textbf{\em{{\fontsize{0.3cm}{1em}\selectfont (On-Body)}}} & \em{fingers} & I.b. fingers.\\ \cline{2-3} 
             & \em{leg} & I.b. finger and leg.\\ \cline{2-3} 
             & \em{back of hand} & I.b. finger and back of hand.\\ \cline{2-3} 
             & \em{forearm} & I.b. finger and forearm.\\ \cline{2-3} 
             & \em{face} & I.b. finger and face.\\ \cline{2-3} 
             & \em{wrist} & I.b. finger and wrist.\\ \cline{2-3} 
             & \em{ring} & I.b. finger and ring. \\ \cline{2-3} 
             & \em{watch} & I.b. finger and watch.\\ \cline{2-3} 
             & \em{glasses} & I.b. finger and glasses.\\ \cline{2-3} 
             & \em{necklace} & I.b. finger and necklace.\\ 
      \Xhline{4\arrayrulewidth}
        \textbf{\em{Binding}} & \em{direct} & Directly control in front of screen. \\ \cline{2-3} 
             & \em{surface} & Absolute mapping screen to surface.\\ \cline{2-3} 
             & \em{independent} & No binding b. screen and control.\\
      \Xhline{4\arrayrulewidth}
        \textbf{\em{Nature}} & \em{symbolic} & Control visually depicts a symbol.\\ \cline{2-3} 
             & \em{physical} & Control acts physically on objects.\\ \cline{2-3} 
             & \em{metaphorical} & Control indicates a metaphor.\\ \cline{2-3} 
             & \em{abstract} & Control mapping is arbitrary.\\
      \Xhline{4\arrayrulewidth}
        \textbf{\em{Flow}} & \em{discrete} & Response occurs \em{after} the user acts.\\ \cline{2-3} 
             & \em{continuous} & Response occurs \em{before} the user acts.\\
      \hline
    \end{tabular}
    \caption{Taxonomy of game controls based on 2448 control actions. The abbreviation ``I.b.'' means ``Interaction between''.}
    \label{tab:taxonomy}
    \end{adjustwidth}
  \end{table}

  The scope of the \emph{Form} dimension is applied separately to different interaction methods. There are 4 form categories with the \emph{In-Air} interaction method. \emph{Finger} is a special case of \emph{hand}, but it is worth distinguishing because of its similarity to mouse actions and direct-touch. There are 11 \emph{Form} categories with \emph{On-Body} input. 4 of them (\emph{palm}, \emph{back of hand}, \emph{forearm}, \emph{wrist}) are performed with both hands. 2 of them (\emph{leg}, \emph{face}) use a single hand to interact with other body parts. \emph{fingers} is a single hand control and it is merely an interaction between fingers, e.g. a pinch. And rest of them (\emph{ring}, \emph{watch}, \emph{glasses}, \emph{necklace}) are interactions with accessories.


  In the \emph{Nature} dimension, \emph{symbolic} controls are visual depiction. For example, a user poses the v-sign in the air in order to select menu option 2, or forms his hand as a gun to throw an object. \emph{physical} controls with the virtual object should be similar to the real world interaction with the physical object. \emph{metaphorical} controls occur when a control acts on, with, or like something else. For instance, users trace a finger in circle to simulate the ``object rotation'', or view the palm as a trackpad to perform gestures. As it should be, the control itself is usually not enough to reveal its metaphorical nature; the answer lies in the user's mental model which could be understood by the interview afterwards. Finally, \emph{abstract} controls have no \emph{symbolic}, \emph{physical}, or \emph{metaphorical} connection to their game task. The mapping is arbitrary, which does not necessarily mean it is poor. Pinch-touching thumb and index finger to perform ``avatar jump'', for example, would be an abstract control.

The \emph{Binding} dimension is defined as the relationship between the control area and the smart glass's screen. \emph{direct} binding means the user performs the controls in the screen region directly, such as using an in-air gesture right in front of the screen to drag or touch virtual objects. A game control binding is called a \emph{surface} binding if the user absolutely maps the screen onto another surface and performs the game controls on it. Dragging a finger on the palm to move the object on the screen, for example, is a \emph{surface} binding control. For controls categorized as \emph{independent} controls it means that there is no binding between the screen and the control area. Thus the control can be performed in any position, like the ``pinch to jump''.

  A game control's \emph{Flow} is \emph{discrete} if the control is performed, delimited, recognized, and responded to as an event. An example is punching in the air to perform ``avatar attack''. \emph{Flow} is \emph{continuous} if ongoing recognition is required, such as during most of our participants' ``3D Camera Control'' rotating the imaginary camera by hands. 
 

 \subsubsection{Taxonometric Breakdown of Control Actions in our Data}
We found that our taxonomy adequately describes even widely differing control actions made by our users. Figure \ref{fig:InAirTaxonomy} and \ref{fig:OnbodyTaxonomy} show for each dimension the percentage distribution between the categories.

The \emph{Form} dimension for \emph{In-Air} input and the the \emph{Form} dimension for \emph{On-Body} input are both dominated with hand related input. We found that the forms of \emph{On-Body} controls are more complicated than those of \emph{In-Air}. Nonetheless, the binding of \emph{On-Body} is more consistent. About 75\% of the controls are independent of the screen. In addition, we were surprised that no user designed a \emph{direct-binding} or \emph{physical} control with \emph{On-Body} input. And we found that for the \emph{Flow} dimension, the percentage distribution is similar for In-Air and On-Body interaction.

 \begin{figure}[!h]
  \centering
  \includegraphics[width=1\columnwidth]{InAirTaxonomy.pdf}
  \caption{Percentage of game controls in each taxonomy dimension with \emph{In-Air} interaction.}
  \label{fig:InAirTaxonomy}
  \end{figure} 

 \begin{figure}[!h]
  \centering
  \includegraphics[width=1\columnwidth]{OnbodyTaxonomy.pdf}
  \caption{Percentage of game controls in each taxonomy dimension with \emph{On-Body} input.}
  \label{fig:OnbodyTaxonomy}
  \end{figure} 

  \subsection{User-Defined Game Control Set}
  At the heart of this work is the creation of a user-defined game control set with smart glasses in real-world environment. This section gives the process by which the set was created and properties of the set. Unlike prior game control sets in traditional game platforms, this set is based on observed user behavior and joins user action to game tasks.

   \subsubsection{Agreement}
   After all 24 participants had provided game control for each game tasks with each glasses form and interaction methods, we grouped the game control within each task such that each group held identical controls. Group size was then used to compute an \emph{Agreement Scrore} that reflects, in a single number, the degree of consensus among participants. A task with .31 agreement score means that, random two users will has 31\% chance to perform the identical control action for this task. (The definition and formula of agreement score were refer to previous work \cite{Wobbrock:2005:MGS:1056808.1057043}.)
   \begin{equation}
   A = \frac{\displaystyle{\sum_{t\epsilon T }} \sum_{P_i \subseteq P_t } \left(\frac{\lvert{P_i}\rvert}{\lvert{P_t}\rvert}\right) ^ 2}{\displaystyle{\lvert{T}\rvert}}
   \end{equation}
  
   In eq. 1, $t$ is a task in the set of all tasks $T$, $P_{t}$ is the set of proposed control actions for task $t$, and $P_i$ is a subset of identical control action from $P_{t}$. The range for $A$ is $\left[\lvert{P_t}\rvert ^{-1}, 1\right]$. As an example, consider agreement for \emph{draw a path} with \emph{On-Body} input, it had four groups of identical control actions with groups size 34, 4, 5 and 5. we compute

   \begin{equation}
   A_{body-path} = \left(\frac{34}{48}\right) ^ 2  + \left(\frac{4}{48}\right) ^ 2 + \left(\frac{5}{48}\right) ^ 2 + \left(\frac{5}{48}\right) ^ 2 = 0.53
   \end{equation}

 Agreement for our study is graphed in Figure \ref{fig:Agreement}. The overall agreement for \emph{In-Air} and \emph{On-Body} inputs were $A_{Air}$=0.27 and $A_{Body}$=0.25, respectively. In comparison to the agreement of \emph{In-Air} and \emph{On-Body} inputs, we found their patterns were extreme similar. The average difference of agreement between these two interaction methods was .056. It implied that the agreement score was influenced more by the game tasks than by the interaction methods.


 \begin{figure}[!h]
  \centering
  \includegraphics[width=1\columnwidth]{Agreement.pdf}

  \caption{Agreement for each game tasks. The tasks are listed in the same order as they appear in Table \ref{tab:table1}.}
  \label{fig:Agreement}
  \end{figure} 

   %\subsubsection{Conflict and Coverage}

   %Howerver, where the same control action was used to perform different game tasks, a conflict occurred if the game consists the tasks at same time. In our case ,we found that ``Move in 4 Directions'',``Avatar 3D Move'' and ``Viewport Controls'' are assigned with same control action with on-body input(Fig?.?). According to the property of our top 90 casual games, the average task count for these game is 2.56 (std=1.07). In another word, there are about 1 to 4 game tasks in each game title. There is a low chance to perform these three game controls at same game and same time. To make our game control set reflects more consistent with user behavior, we decide to remain it conflict. 


   \subsubsection{Properties of the User-defined Game Control Sets}
   The user-defined game control set was developed by taking largest groups with identical control for each game task and then assigned those groups' control to the task. 
   Our results of the user-defined game control set covered game controls with \emph{In-Air} and \emph{On-Body} with 40.07\% and 41.32\% respectively. Our user defined set is useful, therefore, not just for what it contains, but also for what it omits.

   With \emph{In-Air} (see Figure \ref{fig:InAirSet}), although we informed users to perform game control not limited with hands in advance, the result showed that users still preferred to use hand gestures rather than use voice controls, eye gesture and head tilting. Additionally, users would make use of direct-control if they had to perform precise task, such as selecting an object from many or moving an object to the specific position. As other tasks with lower precision requirement, such as selecting a single option from 4 or enabing an avatar jump. Users would prefer using an indirect-control. For example, users tapped 4 different area in front of their chest or raised hands slightly.


    All the controls in our final \emph{On-Body} input set are finger related (see Figure \ref{fig:OnBodyInputSet}). Most of them use single finger tip to perform gestures on different surfaces. And the most preferable surface for on-body gestures is hand palm. These controls seem like an trackpad or a proxy touch-screen. The remained controls are finger interaction with single hand. Specifically, users use thumb to interact with index finger or ring.

   \subsubsection{Taxonometric Breakdown of User-Defined Game Controls}
   As we should expect, the taxonometric breakdown of the final user-defined game control sets(Figure \ref{fig:InAirSet} and \ref{fig:OnBodyInputSet}) are similar to the proportions of all control actions proposed (Figure \ref{fig:InAirTaxonomy} and \ref{fig:OnbodyTaxonomy}). Across all taxonomy categories, the average difference between these two sets were only 3.09\% and 6.31\% respectively.

  \subsection{Mental Model Observations}
    \subsubsection{Social Acceptance and Control Area}
    To our surpirsed, approximately 63\% of in-air gestures were not performed in front of their face (See Figure \ref{fig:figureInAirPorpotion}.2). This behavior conflicted with current ``Google Glass'' design and previous work\cite{Colaco:2013:MCL:2501988.2502042}. There were 7 participants perfoming more gestures in front of face than other position. They indicated that control in front of face was more precise and intuitive. At the same time, the other 17 users preferred to perform in-air gesture in front of their chest or below. Among them, there are 3 participants never performed any in-air gesture in front of their face. These users indicated that moving a finger in front of their face was really weird and not social acceptable. And there was also a hand fatigue problem to lift hand in front of face. They thought it was not suitable for game control.

    \subsubsection{Metaphor from Exisiting Game Control}
    Although we took care not to show elements from traditional game platforms like Consoles, PCs or Mobile games, participants still often thought with the previous gaming experience. For example, some controls actions performed as if using touch-screen in front of their face (see Figure \ref{fig:InAirSet}\{A,F,G\}). And some actions is similiar with using a imaginary trackpad on in-air surface or hand surface(see Figure \ref{fig:InAirSet}\{H,I\} and Figure \ref{fig:OnBodyInputSet}\{B,D,E,H,I,K,N,Q\}). Moreover, an imaginary button on the index finger(Figure \ref{fig:OnBodyInputSet}.M). Even with simple shapes and basic character, it was clear how deeply rooted the previous gaming experience is. Some quotes reveal this: ``just click the button like a joystick,''``can I just imagine there is a trackpad on my palm?''``It's a imaginary touch-screen.''

\subsubsection{Identical Gestures on Different Surfaces}
 In our study, we found several identical gestures performed by our users with different surfaces. Take the task ``Move in 4 directions'' for example, although 57\% of gestures were performed by moving finger on the palm with \emph{On-Body} inputs(Figure \ref{fig:figureOnBodyPorpotion}.2). The rest gestures were mostly using identical gestures on the different surfaces such as handback, leg, forearm and even face. Same phenomena could also be observed by comparing the game control between our two user-defined game control sets. ``Move left and right'', ``Move in 4 directions'', ``Draw a Path'', ``Throw an Object'' were assigned with identical gestures with palm surfaces and in-air imaginary surfaces (See Figure \ref{fig:InAirSet} and Figure \ref{fig:OnBodyInputSet} \{D,E,H,I\}). In these cases, the surfaces did not influence the meaning of gestures, which could be performed on any surfaces. We have asked users why they choosed palm as their input area. The general response was concerning about the lowest physical movement demand, such as ``I choose left palm to perform gesture because it is near to my right index finger''.


  \begin{figure*}
  \centering
  \includegraphics[width=1\textwidth]{InAirSet.pdf}
  \caption{The user-defined game control set with \emph{In-Air} interaction, The percentage indicate the portion of users who perform the control action for the game task.}
  \label{fig:InAirSet}
  \end{figure*}


  \begin{figure*}
  \centering
  \includegraphics[width=1\textwidth]{OnBodyInputSet.pdf}
  \caption{The user-defined game control set with \emph{On-Body} inputs. The percentage indicate the portion of users who perform the control action for the game task. Note that, there are 3 tasks (``Move in 4 directions'',``Avatar 3D Move'',``Viewport Control'') have been assigned with an identical control action.}
  \label{fig:OnBodyInputSet}
  \end{figure*}


  \section{Discussion}
    %\subsubsection{Users' and Designers' Gestures}
    \subsubsection{Implications for In-Air Interaction Technology}
    With \emph{In-Air} interaction methods, our taxonomy shows that finger and hand are still the dominant  forms for smart glass gaming (Figure \ref{fig:figureInAirPorpotion}.1\{A,B\}). there are only a small number to use head-gesture, eye-gesture and voice controls, 7\%, 3\% and 1\% respetively.
    Before the study began, both Google Glass and Epson\cite{GoogleGlass, Colaco:2013:MCL:2501988.2502042} supplied their own in-air gesture sets to increase the diversity of their input. However, the result shows that 63\% in-air gestures are not performed in front of users' faces in the public space due to social acceptance and physical tiring issues metioned by feedback (see Figure \ref{fig:figureInAirPorpotion}.2). Therefore, if the developers of head-worn devices want to implement in-air gestures for input, they will need to have the capablity for sensing gestures in a wide range of areas near users. Take CV-based sensing technologies for example, they can use wide-angle lens or fish-eye lens to carry out a gesture set in order to cater to users' preference.   
  \begin{figure}[!h]
  \centering
  \includegraphics[width=1\columnwidth]{InAirControlArea.pdf}
  \caption{1.The top 4 \emph{In-Air} interaction forms. Percentage and the measure of area indicates the portion of \emph{In-Air} game controls. (A)Using finger to perform in-air gesture. (B)Using full hand to perform in-air gesture. (C)Using head-tilting or rotating to perform game control. (D)Perfrom game control with eyes-gesture. 2.The portion of in-air gesture control area. Half of in-air gestures (49\%) were performed in front of their chest, 14\% were in front of belly or below. Only 37\% gestures were performed in front of their face.}
  \label{fig:figureInAirPorpotion}
  \end{figure}
  \subsubsection{Implications for On-Body Input Technology}
    Our results showed that palm was the most favorite area for users to perform \emph{On-Body} inputs. Half of the game controls with \emph{On-Body} input used a finger to perform gesture on palm (See Figure \ref{fig:figureOnBodyPorpotion}). According to the mental model mentioned above, users utilized the metaphor of trackpad and touch-screen on the palm in several cases. Current gesture-recognizer on trackpad and touch-screen might be a suitible implementation reference for these input controls.

    From another point of view, all these controls were conducted by fingers of users dominant hand. If possible, we suggested that the sensing area should be targeted around the dominant hand finger rather than other specific parts of body, such as palm or leg. Hence, you could sense almost all cases of users' \emph{On-Body} inputs. Take CV-based sensing techonologies for example, we should put the fish-eye lens around the dominant hand finger rather than palm.

  \subsubsection{Implications for Game Design}
  According to the agreement scores we found, no matter using \emph{In-Air} interaction or \emph{On-Body} input, the average agreement between users were only .26, and the highest one was just about .55 (see Figure \ref{fig:Agreement}). In this case, guessing the game control would become a frustrating experience for users. It indicated that game developers should design the visual guide carefully to lead users performimg the control action, or show a instruction to explain the control methods.


 \begin{figure}[!h]
  \centering
  \includegraphics[width=1\columnwidth]{OnBodyForms.pdf}
  \caption{The top 6 \emph{On-Body} input forms. Percentage and the measure of area indicates the portion of \emph{On-Body} game controls. (A)Interact between finger and palm. (B)Interact with ring. (C)Interact between fingers. (D)Interact between finger and leg. (E)Interact between finger and hand back. (F)Interact with watch.}
  \label{fig:figureOnBodyPorpotion}
  \end{figure}   

  %\subsubsection{Implications for User Interfaces}
  \subsubsection{Limitation and Next Steps}

  In our work, we showed the single game task independent to users. Users designed these controls without the overview of all tasks. Our resulting sets might conflicted with each other if we wanted to perform two or three controls simultaneously. For example, with \emph{On-Body} input, users would feel unconfortable to ``Move in 4 directions'' and performed ``Avatar attack'' at the same time (see Figure \ref{fig:OnBodyInputSet}\{E,O\}). In future work, we will try more possibility to show users the combination of multiple tasks and to understand more about how users behavior alter under this circumstance.

  As we know, there are many places known as public space, and users may behave differently in each place. Futhermore, in our study, we did not ask users to define any control actions to interact with tangible objects in public space, such as, tables or chairs in the cafe. We only requested participants to experience two types of smart glasses. Therefore, our user-defined game control sets might not suit to be applied to games on other types of head worn devices. Moreover, our participants were all literate Taiwanese adults; Undoubtedly, childern, elders, Westerners, or uneducated participants would behave differently. That is to say, these issues are worthy of investigation, but exceed the range of our current work.

  An important next step is to validate our user-defined game control set with a wearable system, which can sense all \emph{In-Air} and \emph{On-Body} control actions listed in our set.   
    %Glass Forms 只有兩種
    %Culture問題?
    %Interaction with 桌椅？


\section{Conclusion}

We have presented user-defined game control sets by performing a study of glass game control with participants' agreement over 2448 control actions.
It can reflect the user gaming behavior and habit in public space, such as choosing a relatively unobtrusive area to perform game control. In addition, We firmly believe that our user defined game control set is also a good candidate for development in glass game control system. In another hand, we have presented a taxonomy of glass game controls for analyzing and characterizing control actions with smart glasses. 
In capturing game controls for this study, we have gained insight into the mental models of public users and have tranlated these into implications for techonoloy and design. 
This work represents a necessary step in bringing smart glass gaming closer to the hands and minds of public people. And it also becomes a milestone to help designers create games on smart glasses for better users' experience.

%It is important that you write for the SIGCHI audience.  Please read
%previous years' Proceedings to understand the writing style and
%conventions that successful authors have used.  It is particularly
%important that you state clearly what you have done, not merely what
%you plan to do, and explain how your work is different from previously
%published work, i.e., what is the unique contribution that your work
%makes to the field?  Please consider what the reader will learn from
%your submission, and how they will find your work useful.  If you
%write with these questions in mind, your work is more likely to be
%successful, both in being accepted into the Conference, and in
%influencing the work of our field.

%\section{Acknowledgments}

%We thank CHI, PDC and CSCW volunteers, and all publications support
%and staff, who wrote and provided helpful comments on previous
%versions of this document.  Some of the references cited in this paper
%are included for illustrative purposes only.  \textbf{Don't forget
%to acknowledge funding sources as well}, so you don't wind up
%having to correct it later.

% Balancing columns in a ref list is a bit of a pain because you
% either use a hack like flushend or balance, or manually insert
% a column break.  http://www.tex.ac.uk/cgi-bin/texfaq2html?label=balance
% multicols doesn't work because we're already in two-column mode,
% and flushend isn't awesome, so I choose balance.  See this
% for more info: http://cs.brown.edu/system/software/latex/doc/balance.pdf
%
% Note that in a perfect world balance wants to be in the first
% column of the last page.
%
% If balance doesn't work for you, you can remove that and
% hard-code a column break into the bbl file right before you
% submit:
%
% http://stackoverflow.com/questions/2149854/how-to-manually-equalize-columns-
% in-an-ieee-paper-if-using-bibtex
%
% Or, just remove \balance and give up on balancing the last page.
%
\balance

%\section{References format}
%References must be the same font size as other body text.
% REFERENCES FORMAT
% References must be the same font size as other body text.

\bibliographystyle{acm-sigchi}
\bibliography{sample}
\end{document}
